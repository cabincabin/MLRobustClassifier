{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCP_mlapis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "S5EgSNLvXywU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Google Cloud Platform - Using Machine Learning APIs  ).\n",
        "\n",
        "This is an upgraded Python revision of [this notebook](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/CPB100/lab4c/mlapis.ipynb).\n",
        "\n",
        "This notebook originally was being processed using DataLab on the Google Cloud Platform.  This particular incarnation of the notebook is for running on Google Colaboratory which I am trying out for the first time.\n",
        "\n",
        "### Security\n",
        "\n",
        "First things first - we need to authenticate against the Google Cloud APIs.\n",
        "\n",
        "#### Getting a Google API Credential.\n",
        "\n",
        "First, visit <a href=\"http://console.cloud.google.com/apis\">API console</a>, choose \"Credentials\" on the left-hand menu.  Choose \"Create Credentials\" and generate an API key for your application. You should probably restrict it by IP address to prevent abuse, but for now, just  leave that field blank and delete the API key after trying out this demo.\n",
        "\n",
        "Then, when you have your key, you will enter it in this first executable cell:"
      ]
    },
    {
      "metadata": {
        "id": "WxMSTfPdfoaO",
        "colab_type": "code",
        "outputId": "572f846a-930d-4519-87c1-fb681469e2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "\n",
        "APIKEY = getpass.getpass()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2w4bHiuYXywg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the same API console, choose \"Dashboard\" on the left-hand menu and \"Enable API\".\n",
        "\n",
        "Enable the following APIs for your project (search for them) if they are not already enabled:\n",
        "<ol>\n",
        "<li> Google Translate API </li>\n",
        "<li> Google Cloud Vision API </li>\n",
        "<li> Google Natural Language API </li>\n",
        "<li> Google Cloud Speech API </li>\n",
        "</ol>\n",
        "\n",
        "Finally, because we are calling the APIs from Python (clients in many other languages are available), let's install the Python package (it's not installed by default on Datalab).\n",
        "\n",
        "```!pip install --upgrade pip```\n",
        "\n",
        "```!pip install --upgrade google-api-python-client```\n"
      ]
    },
    {
      "metadata": {
        "id": "iTdwxe76Xywy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Invoke Translate API\n",
        "\n",
        "[Google Cloud Translation](https://cloud.google.com/translate/docs/) documentation.  I know a lot has gone on here - see [my LSTM notebook](https://nbviewer.jupyter.org/github/jeffreyrnorton/Notebooks_MachineLearning/blob/master/DeepNetsWithKeras_ANN_LSTM.ipynb) where I trained a Seq2Seq LSTM network to do translation on a relatively small vocabulary.\n",
        "\n",
        "Also note that this is a service.  The translation is not happening on the VM running the notebook, but is running as a service.  This is where we start seeing the true power of cloud compute!"
      ]
    },
    {
      "metadata": {
        "id": "HjjvlaPkXyw0",
        "colab_type": "code",
        "outputId": "4ecba34a-f4b9-4df0-ab0e-f962baea8540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "cell_type": "code",
      "source": [
        "# running Translate API\n",
        "from googleapiclient.discovery import build\n",
        "service = build('translate', 'v2', developerKey=APIKEY)\n",
        "\n",
        "# use the service\n",
        "inputs = ['is it really this easy?', 'amazing technology', 'wow']\n",
        "outputs = service.translations().list(source='en', target='fr', q=inputs).execute()\n",
        "# print outputs\n",
        "for input, output in zip(inputs, outputs['translations']):\n",
        "  print(u\"{0} -> {1}\".format(input, output['translatedText']))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HttpError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c37600393164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# use the service\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'is it really this easy?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'amazing technology'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# print outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'translations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://translation.googleapis.com/language/translate/v2?source=en&target=fr&q=is+it+really+this+easy%3F&q=amazing+technology&q=wow&key=AIzaSyDuO40WLC3lgV8TL_EukqthnT8N_s8liWY&alt=json returned \"Cloud Translation API has not been used in project 702777444868 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/translate.googleapis.com/overview?project=702777444868 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\">"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tueBLHcXXyw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That is really cool - how would a Gallego (a person from Galicia in the Northwest corner of Spain) say it?"
      ]
    },
    {
      "metadata": {
        "id": "YiUqUjZoXyw8",
        "colab_type": "code",
        "outputId": "e891a641-e942-4e6f-97d8-63aaadd6b925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "outputs = service.translations().list(source='en', target='gl', q=inputs).execute()\n",
        "# print outputs\n",
        "for input, output in zip(inputs, outputs['translations']):\n",
        "  print(u\"{0} -> {1}\".format(input, output['translatedText']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is it really this easy? -> é realmente tan fácil?\n",
            "amazing technology -> tecnoloxía sorprendente\n",
            "wow -> wow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MVqvz5erXyxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Invoke Vision API\n",
        "\n",
        "The [Vision API](https://cloud.google.com/vision/docs/) can work off an image in Cloud Storage or embedded directly into a POST message. I'll use Cloud Storage and do OCR on this image: <img src=\"https://storage.googleapis.com/cloud-training-demos/vision/sign2.jpg\" width=\"200\" />.  \n",
        "That photograph is from http://www.publicdomainpictures.net/view-image.php?image=15842."
      ]
    },
    {
      "metadata": {
        "id": "MeyNji74XyxG",
        "colab_type": "code",
        "outputId": "ce955d7e-a0d5-49e2-b68e-2ce3fbd7fbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "# Running Vision API\n",
        "import base64\n",
        "IMAGE=\"gs://cloud-training-demos/vision/sign2.jpg\"\n",
        "vservice = build('vision', 'v1', developerKey=APIKEY)\n",
        "request = vservice.images().annotate(body={\n",
        "        'requests': [{\n",
        "                'image': {\n",
        "                    'source': {\n",
        "                        'gcs_image_uri': IMAGE\n",
        "                    }\n",
        "                },\n",
        "                'features': [{\n",
        "                    'type': 'TEXT_DETECTION',\n",
        "                    'maxResults': 3,\n",
        "                }]\n",
        "            }],\n",
        "        })\n",
        "responses = request.execute(num_retries=3)\n",
        "print(responses['responses'][0]['textAnnotations'][0]['description'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "请您爱护和保\n",
            "护卫生创建优\n",
            "美水环境\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xo82qdtIXyxO",
        "colab_type": "code",
        "outputId": "2c1e8537-d129-4ff9-c66d-a63eb931e289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "foreigntext = responses['responses'][0]['textAnnotations'][0]['description']\n",
        "foreignlang = responses['responses'][0]['textAnnotations'][0]['locale']\n",
        "print('Language Code = {}.  Foreign Text:\\n{}'.format(foreignlang,foreigntext))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language Code = zh.  Foreign Text:\n",
            "请您爱护和保\n",
            "护卫生创建优\n",
            "美水环境\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Buuu_L0SXyxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate sign\n",
        "\n",
        "I don't read Chinese - what does it say.  Let's run it through the translator."
      ]
    },
    {
      "metadata": {
        "id": "3bte0ys8Xyxa",
        "colab_type": "code",
        "outputId": "3ef1a691-f5e9-4549-9a83-37cf37268ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "inputs=[foreigntext]\n",
        "outputs = service.translations().list(source=foreignlang, target='en', q=inputs).execute()\n",
        "# print outputs\n",
        "for input, output in zip(inputs, outputs['translations']):\n",
        "  print(u\"{0} -> {1}\".format(input, output['translatedText']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "请您爱护和保\n",
            "护卫生创建优\n",
            "美水环境\n",
            " -> Please love and protect hygiene to create a beautiful water environment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xHsP9QTJXyxg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## More OCR with the Vision API\n",
        "OCR intrigues me - it is actually quite difficult to do well and there are engines like Tesseract that aren't too bad.  So I want to try the engine with a POST and see if it can extract some English for me, but on a very difficult sign (text with skew):"
      ]
    },
    {
      "metadata": {
        "id": "dvUj1VwijpvG",
        "colab_type": "code",
        "outputId": "daf56e29-edcd-42d7-e404-100280ef2bd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "cell_type": "code",
      "source": [
        "import IPython.display\n",
        "\n",
        "IPython.display.Image(url=\"https://get.pxhere.com/photo/road-highway-advertising-travel-sign-community-usa-landmark-street-sign-attraction-historic-tourism-signage-road-sign-illinois-history-66-traffic-sign-route-66-mother-road-odell-644464.jpg\",\n",
        "                      width=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://get.pxhere.com/photo/road-highway-advertising-travel-sign-community-usa-landmark-street-sign-attraction-historic-tourism-signage-road-sign-illinois-history-66-traffic-sign-route-66-mother-road-odell-644464.jpg\" width=\"600\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "GbTiAkidXyxi",
        "colab_type": "code",
        "outputId": "82bba057-da54-4c71-8355-780b4667a5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "IMAGE=\"https://get.pxhere.com/photo/road-highway-advertising-travel-sign-community-usa-landmark-street-sign-attraction-historic-tourism-signage-road-sign-illinois-history-66-traffic-sign-route-66-mother-road-odell-644464.jpg\"\n",
        "vservice = build('vision', 'v1', developerKey=APIKEY)\n",
        "request = vservice.images().annotate(body={\n",
        "        'requests': [{\n",
        "                'image': {\n",
        "                    'source': {\n",
        "                        'imageUri': IMAGE\n",
        "                    }\n",
        "                },\n",
        "                'features': [{\n",
        "                    'type': 'TEXT_DETECTION',\n",
        "                    'maxResults': 5,\n",
        "                }]\n",
        "            }],\n",
        "        })\n",
        "responses = request.execute(num_retries=5)\n",
        "print(responses['responses'][0]['textAnnotations'][0]['description'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROUTE\n",
            "ROADSIDE\n",
            "OO ATTRACTION\n",
            "1932 STANDARD OIL GAS STATION, ODELL, ILLINOIS\n",
            "This restoration is a project of the Route 66 Association of\n",
            "on the National Register of Historic Places.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HrDpMknvXyxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Checking the end of the text with the sign - how did we do?  We missed one full line of text - so OCR still remains a difficult text, even for Google!"
      ]
    },
    {
      "metadata": {
        "id": "We0OEsrXXyxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### PDF Document Translation\n",
        "\n",
        "The OCR above works in general on images.  However, there also is a service which operates on PDF documents - as they [say](https://cloud.google.com/vision/docs/ocr), small dense text.\n",
        "\n",
        "Let's try this page from the writings of everybody's favorite fat king eating turkey legs and screaming out \"Call the Executioner\" - King Henry the VIII:\n",
        "![](https://storage.googleapis.com/scoobie_earthquakes/HenryVII_863arabic_0202.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "wqF7wmn6Xyxu",
        "colab_type": "code",
        "outputId": "8d8bbb54-f9a4-4e8c-c0a3-ca8990770508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1025
        }
      },
      "cell_type": "code",
      "source": [
        "IMAGE=\"gs://scoobie_earthquakes/HenryVII_863arabic_0202.jpg\"\n",
        "vservice = build('vision', 'v1', developerKey=APIKEY)\n",
        "request = vservice.images().annotate(body={\n",
        "        'requests': [{\n",
        "                'image': {\n",
        "                    'source': {\n",
        "                        'gcs_image_uri': IMAGE\n",
        "                    }\n",
        "                },\n",
        "                'features': [{\n",
        "                    'type': 'DOCUMENT_TEXT_DETECTION',\n",
        "                    'maxResults': 5,\n",
        "                }]\n",
        "            }],\n",
        "        })\n",
        "responses = request.execute(num_retries=5)\n",
        "print(responses['responses'][0]['textAnnotations'][0]['description'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "202\n",
            "35 HENRY VIII.\n",
            "1544.\n",
            "318. CHAPUys to the QUEEN OP HUNGARY-cont.\n",
            "the notice of the Emperor's declaration against Scotland and the certificate\n",
            "of hostility. And on this they again consulted the King, who sant word\n",
            "that he would make no other promise than that after the Emperor's declara\n",
            "tion be would do as tbe treaty bound bim, and that Chapuys should\n",
            "write to the Emperor to make the declaration as he had promised to do wben\n",
            "authentically advertised of tbe hostility between the King and the Boote.\n",
            "as, they said, he had been, by the King's letters. Answered abortly and\n",
            "brusquely tbat there was no great resson in their demand, and, as he had\n",
            "before shown them, far more occasion for the King to make his declaration\n",
            "first, since the hostility between the Emperor and Holstein preceded that of\n",
            "the King and the Scots, as likewise did the requisition for the King to\n",
            "declare himself; and they ought to make no difficulty, especially when\n",
            "Holstein has often boasted a wish to conquer this realm, affirming that it\n",
            "belonged to him, and these years past had designed an enterprise against it\n",
            "as the King himself advertised Chapuys, for which cause the King ought to\n",
            "bave no regard for the Duke; and as to the King's subjects the Easterlingo\n",
            "here would purchase their property there and undertake recovery of their\n",
            "debts. Told them also that he could do no service in this, being afraid to\n",
            "write of it to the Emperor, especially when, on the Srd ult.. the King\n",
            "bad told him, by two of the Council, that there would be no difficulty\n",
            "about his declaring after the Emperor had declared ; and now they said the\n",
            "contrary. Upon this the Council sent Milord Wryothesley and the Socre-\n",
            "tary to the King, who returned with word that the King avowed his saying\n",
            "the above, but that, since the declaration had not been made at once, and\n",
            "other means of delay were put forward. he might well withdraw that\n",
            "promise; nevertheless, to show that he was a prince of his word and wished\n",
            "to proceed sincerely, he would be content to make the required declaration\n",
            "within six weeks after the Emperor's declaration against the Scots and that\n",
            "he would be advertised by letters from the Emperor himself, and not from\n",
            "Flanders, of the hostility between his Majesty and Holstein, expressly\n",
            "mentioning, as the treaty required, the kind of hostility, (viz. : whether he\n",
            "had invaded or caused to be invaded the countries of the Emperor or\n",
            "had given assistance to some other to do so), the English not holding as\n",
            "sufficient cause for the declaration the simple defiance of the Duke\n",
            "against Flanders (les pays de pardeca qu. pardela ?). Told them he\n",
            "had no express power, but thought that the Emperor would condescend\n",
            "thereto; and he would to-day show them the minute which the\n",
            "Emperor had sent (not mentioning that it came from the Emperor,\n",
            "but giving them to understand that he himself would prepare one). It\n",
            "will suffice to make the certificate in the same form as that sent by the\n",
            "King. In the course of conversation they put forward that they were\n",
            "advertised from several quarters that the Pope had paid 4,000 Italians to\n",
            "aid the King of France against him (Henry ?), and that, in pursuance of\n",
            "the treaty, the Emperor would be bound to declare against His Holiness.\n",
            "Upon Chapuys's saying that it was ridiculous (to think) that the Pope\n",
            "would spend a single penny on such affairs, both for his avarice and other\n",
            "respects, and that that ought to be news from Venice, \" dont leur\n",
            "escripvoit souvent de bien lhourdes,\" they asked if in this the Emperor\n",
            "would not believe letters of a secretary of the Pope himself, and of\n",
            "good personages about the King of France, or that King himself. Chapuys\n",
            "answered that he beld that there were secretaries and others about his\n",
            "Holiness who could write such things, either by fiction or conjecture,\n",
            "snd, as to the King of France and his people, there was in them\n",
            "neither drop nor spark of truth; that it was the French custom,\n",
            "* Ses Vol. XVII., App. B. No. 30.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2WIrguLjXyx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis with Language API\n",
        "\n",
        "Let's evaluate the sentiment of some famous quotes using [Google Cloud Natural Language API](https://cloud.google.com/natural-language/docs/)."
      ]
    },
    {
      "metadata": {
        "id": "0Tqux2TOXyx2",
        "colab_type": "code",
        "outputId": "79c895d3-36f4-4e15-ceb8-ba60f60d6a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "lservice = build('language', 'v1beta1', developerKey=APIKEY)\n",
        "quotes = [\n",
        "  'To succeed, you must have tremendous perseverance, tremendous will.',\n",
        "  'It’s not that I’m so smart, it’s just that I stay with problems longer.',\n",
        "  'Love is quivering happiness.',\n",
        "  'Love is of all passions the strongest, for it attacks simultaneously the head, the heart, and the senses.',\n",
        "  'What difference does it make to the dead, the orphans and the homeless, whether the mad destruction is wrought under the name of totalitarianism or in the holy name of liberty or democracy?',\n",
        "  'When someone you love dies, and you’re not expecting it, you don’t lose her all at once; you lose her in pieces over a long time — the way the mail stops coming, and her scent fades from the pillows and even from the clothes in her closet and drawers. '\n",
        "]\n",
        "for quote in quotes:\n",
        "  response = lservice.documents().analyzeSentiment(\n",
        "    body={\n",
        "      'document': {\n",
        "         'type': 'PLAIN_TEXT',\n",
        "         'content': quote\n",
        "      }\n",
        "    }).execute()\n",
        "  polarity = response['documentSentiment']['polarity']\n",
        "  magnitude = response['documentSentiment']['magnitude']\n",
        "  print('POLARITY=%s MAGNITUDE=%s for %s' % (polarity, magnitude, quote))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POLARITY=1 MAGNITUDE=0.9 for To succeed, you must have tremendous perseverance, tremendous will.\n",
            "POLARITY=-1 MAGNITUDE=0.5 for It’s not that I’m so smart, it’s just that I stay with problems longer.\n",
            "POLARITY=1 MAGNITUDE=0.9 for Love is quivering happiness.\n",
            "POLARITY=1 MAGNITUDE=0.9 for Love is of all passions the strongest, for it attacks simultaneously the head, the heart, and the senses.\n",
            "POLARITY=1 MAGNITUDE=0.2 for What difference does it make to the dead, the orphans and the homeless, whether the mad destruction is wrought under the name of totalitarianism or in the holy name of liberty or democracy?\n",
            "POLARITY=-1 MAGNITUDE=0.4 for When someone you love dies, and you’re not expecting it, you don’t lose her all at once; you lose her in pieces over a long time — the way the mail stops coming, and her scent fades from the pillows and even from the clothes in her closet and drawers. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "860-c_rPXyx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In a [paper](https://arxiv.org/pdf/1010.3003.pdf) published in 2010 by Bollen et al, it was claimed that there was 87% correlation between tweets and the stock market.  In January 2013, the following *false* tweet was sent which [momentarily sent Serepta Therapeutics falling](http://fortune.com/2015/12/07/dataminr-hedge-funds-twitter-data/), but when investors realized the ruse, it quickly recovered.  Let's process the [tweet](http://kiddynamitesworld.com/the-sec-needs-to-arrest-some-people/).\n",
        "\n",
        "\"$SRPT FDA steps in as its 48 weeks results on Eteplirsen results are tainted and have been doctored they believeTrial papers seized by FDA.\""
      ]
    },
    {
      "metadata": {
        "id": "JKQykWnmXyx8",
        "colab_type": "code",
        "outputId": "cfd16d36-9f02-4da8-a319-e0a94d0649a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "quotes = [\n",
        "  '$SRPT FDA steps in as its 48 weeks results on Eteplirsen results are tainted and have been doctored they believeTrial papers seized by FDA.'\n",
        "]\n",
        "for quote in quotes:\n",
        "  response = lservice.documents().analyzeSentiment(\n",
        "    body={\n",
        "      'document': {\n",
        "         'type': 'PLAIN_TEXT',\n",
        "         'content': quote\n",
        "      }\n",
        "    }).execute()\n",
        "  polarity = response['documentSentiment']['polarity']\n",
        "  magnitude = response['documentSentiment']['magnitude']\n",
        "  print('POLARITY=%s MAGNITUDE=%s for %s' % (polarity, magnitude, quote))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POLARITY=-1 MAGNITUDE=0.9 for $SRPT FDA steps in as its 48 weeks results on Eteplirsen results are tainted and have been doctored they believeTrial papers seized by FDA.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7LRa6r8aXyyI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And we see that this *is* a very negative statement - no wonder it impacted the market as Forbes relates."
      ]
    },
    {
      "metadata": {
        "id": "YIDoG6EpXyyK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Speech API </h2>\n",
        "\n",
        "The [Speech API](https://cloud.google.com/speech-to-text/docs/) can work on streaming data, audio content encoded and embedded directly into the POST message, or on a file on Cloud Storage. Pass in this <a href=\"https://storage.googleapis.com/cloud-training-demos/vision/audio.raw\">audio file</a> from Cloud Storage."
      ]
    },
    {
      "metadata": {
        "id": "3ufELN3cXyyS",
        "colab_type": "code",
        "outputId": "2e60ad64-9810-4226-ad0b-d74a7c76c282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sservice = build('speech', 'v1beta1', developerKey=APIKEY)\n",
        "response = sservice.speech().syncrecognize(\n",
        "    body={\n",
        "        'config': {\n",
        "            'encoding': 'LINEAR16',\n",
        "            'sampleRate': 16000\n",
        "        },\n",
        "        'audio': {\n",
        "            'uri': 'gs://cloud-training-demos/vision/audio.raw'\n",
        "            }\n",
        "        }).execute()\n",
        "print(response)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'results': [{'alternatives': [{'transcript': 'how old is the Brooklyn Bridge', 'confidence': 0.98360395}]}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fDSX-f3wXyyY",
        "colab_type": "code",
        "outputId": "a8fa1d97-da17-4ddc-bce2-6c9d08e19f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(response['results'][0]['alternatives'][0]['transcript'])\n",
        "print('Confidence=%f' % response['results'][0]['alternatives'][0]['confidence'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how old is the Brooklyn Bridge\n",
            "Confidence=0.983604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxT5FFqaXyye",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Challenge Exercise\n",
        "\n",
        "Here are a few portraits from the Metropolitan Museum of Art, New York (they are part of a [BigQuery public dataset](https://bigquery.cloud.google.com/dataset/bigquery-public-data:the_met) ):\n",
        "\n",
        "gs://gcs-public-data--met/14295/0.jpg  \n",
        "<img src=\"https://raw.githubusercontent.com/jeffreyrnorton/Notebooks_MachineLearning/master/images/14295.jpg\" width=400>\n",
        "\n",
        "gs://gcs-public-data--met/15091/0.jpg  \n",
        "<img src=\"https://raw.githubusercontent.com/jeffreyrnorton/Notebooks_MachineLearning/master/images/15091.jpg\" width=400>\n",
        "\n",
        "(Two given in the original assignment are not publically available and what good is it to tell you unhappy or happy when you can't see the photo?)\n",
        "\n",
        "Use the Vision API to identify which of these images depict happy people and which ones depict unhappy people.\n",
        "\n",
        "Hint: You will need to look for joyLikelihood and/or sorrowLikelihood from the response."
      ]
    },
    {
      "metadata": {
        "id": "_6JEstlzXyyi",
        "colab_type": "code",
        "outputId": "764af3d3-006f-4b3b-9f2d-473ec51e1b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "IMAGE=\"gs://gcs-public-data--met/14295/0.jpg\"\n",
        "vservice = build('vision', 'v1', developerKey=APIKEY)\n",
        "request = vservice.images().annotate(body={\n",
        "        'requests': [{\n",
        "                'image': {\n",
        "                    'source': {\n",
        "                        'gcsImageUri': IMAGE\n",
        "                    }\n",
        "                },\n",
        "                'features': [{\n",
        "                    'type': 'FACE_DETECTION'\n",
        "                }]\n",
        "            }],\n",
        "        })\n",
        "responses = request.execute()\n",
        "print(responses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'responses': [{'faceAnnotations': [{'boundingPoly': {'vertices': [{'x': 1574, 'y': 1151}, {'x': 1760, 'y': 1151}, {'x': 1760, 'y': 1367}, {'x': 1574, 'y': 1367}]}, 'fdBoundingPoly': {'vertices': [{'x': 1589, 'y': 1199}, {'x': 1748, 'y': 1199}, {'x': 1748, 'y': 1358}, {'x': 1589, 'y': 1358}]}, 'landmarks': [{'type': 'LEFT_EYE', 'position': {'x': 1657.5417, 'y': 1239.4186, 'z': -0.00067241676}}, {'type': 'RIGHT_EYE', 'position': {'x': 1709.117, 'y': 1269.9839, 'z': 6.37109}}, {'type': 'LEFT_OF_LEFT_EYEBROW', 'position': {'x': 1648.5151, 'y': 1221.4691, 'z': 3.180908}}, {'type': 'RIGHT_OF_LEFT_EYEBROW', 'position': {'x': 1679.7795, 'y': 1238.1575, 'z': -9.626153}}, {'type': 'LEFT_OF_RIGHT_EYEBROW', 'position': {'x': 1703.0846, 'y': 1251.854, 'z': -6.6261573}}, {'type': 'RIGHT_OF_RIGHT_EYEBROW', 'position': {'x': 1730.7343, 'y': 1266.6852, 'z': 13.025247}}, {'type': 'MIDPOINT_BETWEEN_EYES', 'position': {'x': 1685.7417, 'y': 1255.671, 'z': -8.505504}}, {'type': 'NOSE_TIP', 'position': {'x': 1670.6033, 'y': 1286.2218, 'z': -23.763466}}, {'type': 'UPPER_LIP', 'position': {'x': 1660.6646, 'y': 1302.7893, 'z': -9.950831}}, {'type': 'LOWER_LIP', 'position': {'x': 1651.4833, 'y': 1317.9097, 'z': -5.3513894}}, {'type': 'MOUTH_LEFT', 'position': {'x': 1637.2349, 'y': 1296.7627, 'z': 2.8874674}}, {'type': 'MOUTH_RIGHT', 'position': {'x': 1676.45, 'y': 1317.8884, 'z': 8.183346}}, {'type': 'MOUTH_CENTER', 'position': {'x': 1656.6819, 'y': 1308.821, 'z': -5.574534}}, {'type': 'NOSE_BOTTOM_RIGHT', 'position': {'x': 1680.8508, 'y': 1295.7554, 'z': -0.29969272}}, {'type': 'NOSE_BOTTOM_LEFT', 'position': {'x': 1654.1652, 'y': 1280.362, 'z': -3.7201083}}, {'type': 'NOSE_BOTTOM_CENTER', 'position': {'x': 1665.4689, 'y': 1292.9683, 'z': -10.549813}}, {'type': 'LEFT_EYE_TOP_BOUNDARY', 'position': {'x': 1661.5542, 'y': 1239.922, 'z': -3.6543229}}, {'type': 'LEFT_EYE_RIGHT_CORNER', 'position': {'x': 1667.828, 'y': 1248.5437, 'z': 1.7114741}}, {'type': 'LEFT_EYE_BOTTOM_BOUNDARY', 'position': {'x': 1656.2124, 'y': 1245.564, 'z': -0.05742309}}, {'type': 'LEFT_EYE_LEFT_CORNER', 'position': {'x': 1649.1272, 'y': 1237.3911, 'z': 4.1993794}}, {'type': 'LEFT_EYE_PUPIL', 'position': {'x': 1658.7307, 'y': 1242.7916, 'z': -1.2312143}}, {'type': 'RIGHT_EYE_TOP_BOUNDARY', 'position': {'x': 1710.4882, 'y': 1266.7526, 'z': 2.3725884}}, {'type': 'RIGHT_EYE_RIGHT_CORNER', 'position': {'x': 1717.5399, 'y': 1274.914, 'z': 12.646767}}, {'type': 'RIGHT_EYE_BOTTOM_BOUNDARY', 'position': {'x': 1706.6422, 'y': 1274.9198, 'z': 6.0593023}}, {'type': 'RIGHT_EYE_LEFT_CORNER', 'position': {'x': 1698.7798, 'y': 1266.1927, 'z': 5.4502664}}, {'type': 'RIGHT_EYE_PUPIL', 'position': {'x': 1708.8982, 'y': 1270.3594, 'z': 4.882201}}, {'type': 'LEFT_EYEBROW_UPPER_MIDPOINT', 'position': {'x': 1667.6608, 'y': 1224.7739, 'z': -7.2789946}}, {'type': 'RIGHT_EYEBROW_UPPER_MIDPOINT', 'position': {'x': 1720.6747, 'y': 1253.8289, 'z': -0.72790676}}, {'type': 'LEFT_EAR_TRAGION', 'position': {'x': 1609.7196, 'y': 1244.4164, 'z': 66.17216}}, {'type': 'RIGHT_EAR_TRAGION', 'position': {'x': 1720.8135, 'y': 1305.17, 'z': 79.89174}}, {'type': 'FOREHEAD_GLABELLA', 'position': {'x': 1691.7561, 'y': 1245.1234, 'z': -10.219888}}, {'type': 'CHIN_GNATHION', 'position': {'x': 1638.0819, 'y': 1340.1458, 'z': 3.4793158}}, {'type': 'CHIN_LEFT_GONION', 'position': {'x': 1599.489, 'y': 1280.6748, 'z': 45.39207}}, {'type': 'CHIN_RIGHT_GONION', 'position': {'x': 1699.833, 'y': 1335.5272, 'z': 57.7918}}], 'rollAngle': 28.898441, 'panAngle': 6.19284, 'tiltAngle': -1.9469476, 'detectionConfidence': 0.52649903, 'landmarkingConfidence': 0.40494683, 'joyLikelihood': 'LIKELY', 'sorrowLikelihood': 'VERY_UNLIKELY', 'angerLikelihood': 'VERY_UNLIKELY', 'surpriseLikelihood': 'VERY_UNLIKELY', 'underExposedLikelihood': 'VERY_UNLIKELY', 'blurredLikelihood': 'VERY_UNLIKELY', 'headwearLikelihood': 'VERY_LIKELY'}]}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VpR9BzotXyyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a matter of interest - we can crop to the face with the following code:\n",
        "```\n",
        "xlow = ylow = 100000\n",
        "xhigh = yhigh = -1\n",
        "for point in responses['responses'][0]['faceAnnotations'][0]['boundingPoly']['vertices']:\n",
        "    x = point['x']\n",
        "    y = point['y']\n",
        "    if x < xlow: xlow = x\n",
        "    if y < ylow: ylow = y\n",
        "    if x > xhigh: xhigh = x\n",
        "    if y > yhigh: yhigh = y\n",
        "\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jeffreyrnorton/Notebooks_MachineLearning/master/images/14295.jpg\"\n",
        "response = urllib.request.urlretrieve(url, \"tmp/i.jpg\")\n",
        "img = Image.open(\"tmp/i.jpg\")\n",
        "img2 = img.crop((xlow, ylow, xhigh, yhigh))\n",
        "img2.save(\"tmp/img2.jpg\")\n",
        "```\n",
        "\n",
        "But of course, what we are really interested in are the emotions which we can print out."
      ]
    },
    {
      "metadata": {
        "id": "nKMCKyeUXyyy",
        "colab_type": "code",
        "outputId": "fe8a4cf5-0b6a-45ff-f5e9-af018f77ae68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "annotation = responses['responses'][0]['faceAnnotations'][0]\n",
        "print('Surprise: {}, Joy: {}, Sorrow: {}'.format(annotation['surpriseLikelihood'], annotation['joyLikelihood'], annotation['sorrowLikelihood']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Surprise: VERY_UNLIKELY, Joy: LIKELY, Sorrow: VERY_UNLIKELY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mrMjeMXUXyy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jeffreyrnorton/Notebooks_MachineLearning/master/images/14295.jpg\" width=400>\n",
        "\n",
        "Now that we have explored this - let's write the code to process the other image very concisely and as a function where we assume we are always getting the image from a Google bucket."
      ]
    },
    {
      "metadata": {
        "id": "ZiljD_4SXyy4",
        "colab_type": "code",
        "outputId": "262c47c8-d68e-4366-d038-e55a059ba8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def emotion_detector(apikey, image):\n",
        "    vservice = build('vision', 'v1', developerKey=apikey)\n",
        "    request = vservice.images().annotate(body={\n",
        "        'requests': [{\n",
        "                'image': {\n",
        "                    'source': {\n",
        "                        'gcsImageUri': image\n",
        "                    }\n",
        "                },\n",
        "                'features': [{\n",
        "                    'type': 'FACE_DETECTION'\n",
        "                }]\n",
        "            }],\n",
        "        })\n",
        "    responses = request.execute()\n",
        "    annotation = responses['responses'][0]['faceAnnotations'][0]\n",
        "    return {'Surprise':annotation['surpriseLikelihood'],\n",
        "            'Joy':annotation['joyLikelihood'],\n",
        "            'Sorrow':annotation['sorrowLikelihood']}\n",
        "\n",
        "IMAGE=\"gs://gcs-public-data--met/14295/0.jpg\"\n",
        "print(emotion_detector(APIKEY, IMAGE))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Surprise': 'VERY_UNLIKELY', 'Joy': 'LIKELY', 'Sorrow': 'VERY_UNLIKELY'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fnOOtibaXyy6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jeffreyrnorton/Notebooks_MachineLearning/master/images/15091.jpg\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "TVqCm0C8Xyy6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Clean up </h2>\n",
        "\n",
        "Remember to delete the API key by visiting <a href=\"http://console.cloud.google.com/apis\">API console</a>.\n",
        "\n",
        "If necessary, commit all your notebooks to git.\n",
        "\n",
        "If you are running Datalab on a Compute Engine VM or delegating to one, remember to stop or shut it down so that you are not charged.\n"
      ]
    },
    {
      "metadata": {
        "id": "KMgp1T0FXyy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2018 Google Inc.\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "DpySj3XUXyzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}